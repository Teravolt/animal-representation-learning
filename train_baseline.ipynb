{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using Google Collab, you can import the following:\n",
    "# %pip install -U datasets transformers accelerate ftfy pyarrow wandb pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "from torchvision import transforms\n",
    "import wandb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import GradientAccumulationPlugin\n",
    "from accelerate.utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\n",
    "    'cuda' if torch.cuda.is_available() \\\n",
    "        else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "# DEVICE = 'cpu'\n",
    "\n",
    "CONFIG = Namespace(\n",
    "    run_name='animal-classifier',\n",
    "    model_name='animal-classifier-model-v1',\n",
    "    image_size=256,\n",
    "    hidden_dims=256,\n",
    "    horizontal_flip_prob=0.5,\n",
    "    gaussian_blur_kernel_size=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=15,\n",
    "    learning_rate=4e-4,\n",
    "    seed=1,\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    lr_exp_schedule_gamma=0.85,\n",
    "    lr_warmup_steps=500,\n",
    "    train_limit=-1,\n",
    "    save_model=False,\n",
    "    mixed_precision=None,\n",
    "    grad_accumulation_steps=4\n",
    "    )\n",
    "CONFIG.device = DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "\n",
    "For now, I am using the following data augmentations:\n",
    "- RandomHorizontalFlip - Randomly flips the image horizontally\n",
    "- GaussianBlur - Smooth/blur image using a Gaussian filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(config: Namespace):\n",
    "    \"\"\"\n",
    "    Prepare dataloader\n",
    "    \"\"\"\n",
    "\n",
    "    preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((config.image_size, config.image_size)),  # Resize\n",
    "            transforms.RandomHorizontalFlip(p=config.horizontal_flip_prob),\n",
    "            transforms.GaussianBlur(kernel_size=config.gaussian_blur_kernel_size),\n",
    "            transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
    "            transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n",
    "        ])\n",
    "    \n",
    "    # For pre-processing original image for visualization in W&Bs\n",
    "    preprocess_original = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((512, 512)),  # Resize\n",
    "            transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
    "        ])\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = load_dataset('cats_vs_dogs')\n",
    "    # Remove images that are 100x100 or below.\n",
    "    dataset = \\\n",
    "        dataset.filter(\n",
    "            lambda example: example['image'].size[0] > 100 and example['image'].size[1] > 100)\n",
    "\n",
    "    def transform(examples):\n",
    "        images = [preprocess(image.convert('RGB')) for image in examples['image']]\n",
    "        original_images = [\n",
    "            preprocess_original(image.convert('RGB')) \\\n",
    "                for image in examples['image']]\n",
    "\n",
    "        return {'image': images,\n",
    "                'label': examples['labels'],\n",
    "                'original-image': original_images\n",
    "                }\n",
    "\n",
    "    # Split dataset into train + val. Balance train + val\n",
    "    num_points = len(dataset['train'])\n",
    "    labels = dataset['train']['labels']\n",
    "\n",
    "    split_df = pd.DataFrame()\n",
    "    split_df['labels'] = labels\n",
    "    split_df['id'] = list(range(num_points))\n",
    "    split_df['fold'] = -1\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=config.seed)\n",
    "    for i, (_, test_ids) in enumerate(cv.split(np.zeros(num_points), labels)):\n",
    "        split_df.loc[test_ids, ['fold']] = i\n",
    "\n",
    "    split_df['split'] = 'train'\n",
    "    split_df.loc[split_df.fold == 0, ['split']] = 'val'\n",
    "\n",
    "    # print(split_df[split_df['split'].str.fullmatch('train')].labels.value_counts())\n",
    "    # print(split_df[split_df['split'].str.fullmatch('val')].labels.value_counts())\n",
    "\n",
    "    train_indices = split_df[split_df['split'].str.fullmatch('train')]['id']\n",
    "    val_indices = split_df[split_df['split'].str.fullmatch('val')]['id']\n",
    "\n",
    "    def train_generator():\n",
    "        for idx in train_indices:\n",
    "            yield dataset['train'][idx]\n",
    "\n",
    "    def val_generator():\n",
    "        for idx in val_indices:\n",
    "            yield dataset['train'][idx]\n",
    "\n",
    "    train_dataset = Dataset.from_generator(train_generator)\n",
    "    val_dataset = Dataset.from_generator(val_generator)\n",
    "\n",
    "    train_dataset.set_transform(transform)\n",
    "    val_dataset.set_transform(transform)\n",
    "\n",
    "    train_gen = torch.Generator().manual_seed(config.seed)\n",
    "    val_gen = torch.Generator().manual_seed(config.seed)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=config.per_device_train_batch_size,\n",
    "        shuffle=True, generator=train_gen)\n",
    "    \n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=config.per_device_eval_batch_size,\n",
    "        shuffle=False, generator=val_gen)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, dims: int,\n",
    "                 num_labels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv2d(\n",
    "            in_channels, dims, kernel_size=12)\n",
    "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "        self.conv_2 = torch.nn.Conv2d(\n",
    "            dims, 2*dims, kernel_size=5)\n",
    "        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "        self.conv_3 = torch.nn.Conv2d(\n",
    "            2*dims, 2*dims, kernel_size=3)\n",
    "        self.max_pool_3 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv_4 = torch.nn.Conv2d(\n",
    "            2*dims, 2*dims, kernel_size=3)\n",
    "        self.max_pool_4 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.projection = torch.nn.LazyLinear(4*dims)\n",
    "\n",
    "        self.linear = torch.nn.Linear(4*dims, num_labels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \"\"\"\n",
    "\n",
    "        # print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "        x_ = self.conv_1(x)\n",
    "        x_ = self.max_pool_1(x_)\n",
    "        # print(f\"Output of conv & max pool 1: {x_.shape}\")\n",
    "\n",
    "        x_ = self.conv_2(x_)\n",
    "        x_ = self.max_pool_2(x_)\n",
    "        # print(f\"Output of conv & max pool 2: {x_.shape}\")\n",
    "\n",
    "        x_ = self.conv_3(x_)\n",
    "        x_ = self.max_pool_3(x_)\n",
    "        # print(f\"Output of conv & max pool 3: {x_.shape}\")\n",
    "\n",
    "        x_ = self.conv_4(x_)\n",
    "        x_ = self.max_pool_4(x_)\n",
    "        # print(f\"Output of conv & max pool 4: {x_.shape}\")\n",
    "\n",
    "        x_ = self.flatten(x_)\n",
    "        # print(f\"Output of flatten: {x_.shape}\")\n",
    "\n",
    "        x_ = self.projection(x_)\n",
    "        # print(f\"Output of projection: {x_.shape}\")\n",
    "\n",
    "        output = self.linear(x_)\n",
    "        # print(f\"Final output: {output.shape}\")\n",
    "        return output\n",
    "\n",
    "def create_model(in_dimensions: int, dims: int, num_labels: int):\n",
    "    \"\"\"\n",
    "    Create model\n",
    "    \"\"\"\n",
    "\n",
    "    model = AnimalClassifier(in_dimensions, dims, num_labels)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(preds: torch.Tensor, labels: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Sum over each subset & average over each batch\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    # Cross entropy loss require (batch_size, num_classes, ...)\n",
    "    loss = loss_fn(preds, labels)\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loop(epoch: int, model, dataloader,\n",
    "              wandb_run, accelerator: Accelerator):\n",
    "    \"\"\"\n",
    "    Evaluation loop\n",
    "    \"\"\"\n",
    "\n",
    "    tensor_to_pil = transforms.ToPILImage()\n",
    "\n",
    "    columns = ['pred']\n",
    "\n",
    "    dataframe = []\n",
    "    original_images = []\n",
    "    images = []\n",
    "    gt = []\n",
    "\n",
    "    avg_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        logits = model(batch['image'])\n",
    "        labels = batch['label']\n",
    "\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        loss = compute_loss(logits, labels)\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        # acc = (preds == labels).double()\n",
    "        # print(f\"Accuracy: {acc.mean().item()} - Val loss: {loss.item()}\")\n",
    "        # wandb_run.log({'accuracy': acc.mean()}, commit=False)\n",
    "        # wandb_run.log({'val-loss': loss.item()}, commit=False)\n",
    "        \n",
    "        _images = []\n",
    "        _original_images = []\n",
    "        for j in range(batch['image'].shape[0]):\n",
    "            _images.append(tensor_to_pil(batch['image'][j,:]))\n",
    "            _original_images.append(tensor_to_pil(batch['original-image'][j,:]))\n",
    "\n",
    "        images += _images\n",
    "        original_images += _original_images\n",
    "\n",
    "        dataframe += preds.tolist()\n",
    "        gt += batch['label'].tolist()\n",
    "\n",
    "    dataframe = pd.DataFrame(dataframe,\n",
    "                             columns=columns)\n",
    "    dataframe['epoch'] = epoch\n",
    "    dataframe['image'] = \\\n",
    "        [wandb.Image(image) for image in images]\n",
    "    dataframe['original_images'] = \\\n",
    "        [wandb.Image(image) for image in original_images]\n",
    "    dataframe['gt'] = gt\n",
    "\n",
    "    # Get average accuracy and loss\n",
    "    acc = (dataframe['gt'] == dataframe['pred']).mean()\n",
    "    avg_loss = avg_loss/len(dataloader)\n",
    "\n",
    "    accelerator.print(\n",
    "        f\"Val accuracy and loss: {acc} - {avg_loss}\")\n",
    "\n",
    "    table = wandb.Table(data=dataframe)\n",
    "    wandb_run.log({'accuracy': acc}, commit=False)\n",
    "    wandb_run.log({'val-loss': loss}, commit=False)\n",
    "    wandb_run.log({'eval-table': table})\n",
    "\n",
    "def training_loop(config: Namespace):\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "    \"\"\"\n",
    "\n",
    "    wandb_run = wandb.init(project='Animal-Classifier', entity=None,\n",
    "                           job_type='training',\n",
    "                           name=config.run_name,\n",
    "                           config=config)\n",
    "\n",
    "    set_seed(config.seed)\n",
    "\n",
    "    grad_accumulation_plugin = GradientAccumulationPlugin(\n",
    "        num_steps=config.grad_accumulation_steps,\n",
    "        adjust_scheduler=True,\n",
    "        sync_with_dataloader=True)\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_plugin=grad_accumulation_plugin,\n",
    "        cpu=(config.device == 'cpu'))\n",
    "\n",
    "    train_dataloader, val_dataloader = prepare_dataloader(config)    \n",
    "    model = create_model(3, config.hidden_dims, 2)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=config.lr_warmup_steps)\n",
    "        # last_epoch=config.num_train_epochs*len(train_dataloader))\n",
    "\n",
    "    model, optimizer, train_dataloader, val_dataloader, scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, val_dataloader, scheduler)\n",
    "\n",
    "    num_steps = 0\n",
    "    for epoch in range(config.num_train_epochs):\n",
    "        model.train()\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch}\")\n",
    "\n",
    "        epoch_loss = 0\n",
    "        num_iters = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for _, batch in enumerate(train_dataloader):\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                logits = model(batch['image'])\n",
    "                labels = batch['label']\n",
    "\n",
    "                loss = compute_loss(logits, labels)\n",
    "\n",
    "                # accelerator.print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                wandb_run.log({'loss': loss.item()}, commit=False, step=num_steps)\n",
    "                wandb_run.log({'lr': scheduler.get_lr()[0]}, commit=False, step=num_steps)\n",
    "\n",
    "                num_steps += 1\n",
    "                num_iters += 1\n",
    "\n",
    "                # Update the model parameters with the optimizer\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        # Validate model\n",
    "        eval_loop(epoch, model, val_dataloader, wandb_run, accelerator)\n",
    "\n",
    "        wandb_run.log({'epoch-loss': epoch_loss/num_iters})\n",
    "\n",
    "    if config.save_model:\n",
    "        # Save model to W&Bs\n",
    "        model_art = wandb.Artifact(config.model_name, type='model')\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "        model_art.add_file('model.pt')\n",
    "        wandb_run.log_artifact(model_art)\n",
    "    wandb_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For debugging\n",
    "# MODEL = create_model(3, CONFIG.hidden_dims, 2)\n",
    "# train_dataloader, val_dataloader = prepare_dataloader(CONFIG)\n",
    "# eval_loop(0, MODEL, val_dataloader, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_loop, (CONFIG, ), num_processes=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animal-representation-learning-yALp4Exd-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
