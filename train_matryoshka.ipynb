{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:23:38.228745Z","iopub.status.busy":"2024-02-26T23:23:38.228414Z","iopub.status.idle":"2024-02-26T23:24:02.366618Z","shell.execute_reply":"2024-02-26T23:24:02.365333Z","shell.execute_reply.started":"2024-02-26T23:23:38.228717Z"},"trusted":true},"outputs":[],"source":["# If you are using Google Collab, you can import the following:\n","%pip install -U datasets transformers accelerate ftfy pyarrow wandb pandas numpy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:24:02.374131Z","iopub.status.busy":"2024-02-26T23:24:02.373796Z","iopub.status.idle":"2024-02-26T23:24:07.966394Z","shell.execute_reply":"2024-02-26T23:24:07.965444Z","shell.execute_reply.started":"2024-02-26T23:24:02.374094Z"},"trusted":true},"outputs":[],"source":["from argparse import Namespace\n","\n","from datasets import load_dataset\n","from datasets import Dataset\n","\n","import torch\n","\n","# from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","# from torch.optim.lr_scheduler import CosineAnnealingLR\n","from torch.optim.lr_scheduler import ExponentialLR\n","\n","from torchvision import transforms\n","import wandb\n","\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import StratifiedKFold"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:24:07.968915Z","iopub.status.busy":"2024-02-26T23:24:07.967862Z","iopub.status.idle":"2024-02-26T23:24:08.357889Z","shell.execute_reply":"2024-02-26T23:24:08.357057Z","shell.execute_reply.started":"2024-02-26T23:24:07.968873Z"},"trusted":true},"outputs":[],"source":["from accelerate import Accelerator\n","from accelerate.utils import GradientAccumulationPlugin\n","from accelerate.utils import set_seed"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:24:08.361228Z","iopub.status.busy":"2024-02-26T23:24:08.360923Z","iopub.status.idle":"2024-02-26T23:24:08.369356Z","shell.execute_reply":"2024-02-26T23:24:08.368424Z","shell.execute_reply.started":"2024-02-26T23:24:08.361205Z"},"trusted":true},"outputs":[],"source":["from accelerate.utils import write_basic_config\n","\n","write_basic_config()"]},{"cell_type":"markdown","metadata":{},"source":["## Set up config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:24:08.370619Z","iopub.status.busy":"2024-02-26T23:24:08.370387Z","iopub.status.idle":"2024-02-26T23:24:08.401602Z","shell.execute_reply":"2024-02-26T23:24:08.400484Z","shell.execute_reply.started":"2024-02-26T23:24:08.370598Z"},"trusted":true},"outputs":[],"source":["DEVICE = torch.device(\n","    'cuda' if torch.cuda.is_available() \\\n","        else 'mps' if torch.backends.mps.is_available() else 'cpu')\n","# DEVICE = 'cpu'\n","\n","CONFIG = Namespace(\n","    run_name='animal-classifier',\n","    model_name='animal-classifier-model-v1',\n","    image_size=256,\n","    hidden_dims=256,\n","    horizontal_flip_prob=0.5,\n","    gaussian_blur_kernel_size=3,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=15,\n","    learning_rate=4e-4,\n","    seed=1,\n","    beta_schedule='squaredcos_cap_v2',\n","    lr_exp_schedule_gamma=0.85,\n","    lr_warmup_steps=500,\n","    train_limit=-1,\n","    save_model=True,\n","    mixed_precision=None,\n","    grad_accumulation_steps=4\n","    )\n","CONFIG.device = DEVICE"]},{"cell_type":"markdown","metadata":{},"source":["## Create Dataset\n","\n","For now, I am using the following data augmentations:\n","- RandomHorizontalFlip - Randomly flips the image horizontally\n","- GaussianBlur - Smooth/blur image using a Gaussian filter"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:24:08.403003Z","iopub.status.busy":"2024-02-26T23:24:08.402730Z","iopub.status.idle":"2024-02-26T23:24:08.420671Z","shell.execute_reply":"2024-02-26T23:24:08.419835Z","shell.execute_reply.started":"2024-02-26T23:24:08.402979Z"},"trusted":true},"outputs":[],"source":["def prepare_dataloader(config: Namespace):\n","    \"\"\"\n","    Prepare dataloader\n","    \"\"\"\n","\n","    preprocess = transforms.Compose(\n","        [\n","            transforms.Resize((config.image_size, config.image_size)),  # Resize\n","            transforms.RandomHorizontalFlip(p=config.horizontal_flip_prob),\n","            transforms.GaussianBlur(kernel_size=config.gaussian_blur_kernel_size),\n","            transforms.ToTensor(),  # Convert to tensor (0, 1)\n","            transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n","        ])\n","    \n","    # For pre-processing original image for visualization in W&Bs\n","    preprocess_original = transforms.Compose(\n","        [\n","            transforms.Resize((512, 512)),  # Resize\n","            transforms.ToTensor(),  # Convert to tensor (0, 1)\n","        ])\n","\n","\n","    # Load dataset\n","    dataset = load_dataset('cats_vs_dogs')\n","    # Remove images that are 100x100 or below.\n","    dataset = \\\n","        dataset.filter(\n","            lambda example: example['image'].size[0] > 100 and example['image'].size[1] > 100)\n","\n","    def transform(examples):\n","        images = [preprocess(image.convert('RGB')) for image in examples['image']]\n","        original_images = [\n","            preprocess_original(image.convert('RGB')) \\\n","                for image in examples['image']]\n","\n","        return {'image': images,\n","                'label': examples['labels'],\n","                'original-image': original_images\n","                }\n","\n","    # Split dataset into train + val. Balance train + val\n","    num_points = len(dataset['train'])\n","    labels = dataset['train']['labels']\n","\n","    split_df = pd.DataFrame()\n","    split_df['labels'] = labels\n","    split_df['id'] = list(range(num_points))\n","    split_df['fold'] = -1\n","\n","    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=config.seed)\n","    for i, (_, test_ids) in enumerate(cv.split(np.zeros(num_points), labels)):\n","        split_df.loc[test_ids, ['fold']] = i\n","\n","    split_df['split'] = 'train'\n","    split_df.loc[split_df.fold == 0, ['split']] = 'val'\n","\n","    # print(split_df[split_df['split'].str.fullmatch('train')].labels.value_counts())\n","    # print(split_df[split_df['split'].str.fullmatch('val')].labels.value_counts())\n","\n","    train_indices = split_df[split_df['split'].str.fullmatch('train')]['id']\n","    val_indices = split_df[split_df['split'].str.fullmatch('val')]['id']\n","\n","    def train_generator():\n","        for idx in train_indices:\n","            yield dataset['train'][idx]\n","\n","    def val_generator():\n","        for idx in val_indices:\n","            yield dataset['train'][idx]\n","\n","    train_dataset = Dataset.from_generator(train_generator)\n","    val_dataset = Dataset.from_generator(val_generator)\n","\n","    train_dataset.set_transform(transform)\n","    val_dataset.set_transform(transform)\n","\n","    train_gen = torch.Generator().manual_seed(config.seed)\n","    val_gen = torch.Generator().manual_seed(config.seed)\n","\n","    train_dataloader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=config.per_device_train_batch_size,\n","        shuffle=True, generator=train_gen)\n","    \n","    val_dataloader = torch.utils.data.DataLoader(\n","        val_dataset, batch_size=config.per_device_eval_batch_size,\n","        shuffle=True, generator=val_gen)\n","\n","    return train_dataloader, val_dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:24:08.421922Z","iopub.status.busy":"2024-02-26T23:24:08.421667Z","iopub.status.idle":"2024-02-26T23:24:08.438889Z","shell.execute_reply":"2024-02-26T23:24:08.437781Z","shell.execute_reply.started":"2024-02-26T23:24:08.421899Z"},"trusted":true},"outputs":[],"source":["class AnimalClassifierMatryoshka(torch.nn.Module):\n","\n","    def __init__(self, in_channels: int, dims: int,\n","                 num_labels: int):\n","        super().__init__()\n","\n","        self.conv_1 = torch.nn.Conv2d(\n","            in_channels, dims, kernel_size=12)\n","        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=3)\n","\n","        self.conv_2 = torch.nn.Conv2d(\n","            dims, 2*dims, kernel_size=5)\n","        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=3)\n","\n","        self.conv_3 = torch.nn.Conv2d(\n","            2*dims, 2*dims, kernel_size=3)\n","        self.max_pool_3 = torch.nn.MaxPool2d(kernel_size=2)\n","\n","        self.conv_4 = torch.nn.Conv2d(\n","            2*dims, 2*dims, kernel_size=3)\n","        self.max_pool_4 = torch.nn.MaxPool2d(kernel_size=2)\n","\n","        self.flatten = torch.nn.Flatten()\n","        self.projection = torch.nn.LazyLinear(4*dims)\n","\n","        num_subsets = int(np.log2(dims)) + 2\n","        # print(f\"Number of subsets: {num_subsets} - Dimensions: {4*dims}\")\n","\n","        self.linear_layers = torch.nn.ModuleList()\n","        for i in range(3, num_subsets+1):\n","            # print(f\"Number of dimensions: {2**i}\")\n","            self.linear_layers.append(\n","                torch.nn.Linear(2**i, num_labels))\n","\n","    def forward(self, x: torch.Tensor):\n","        \"\"\"\n","        Forward pass\n","        \"\"\"\n","\n","        x_ = self.conv_1(x)\n","        x_ = self.max_pool_1(x_)\n","        # print(f\"Output of conv & max pool 1: {x_.shape}\")\n","\n","        x_ = self.conv_2(x_)\n","        x_ = self.max_pool_2(x_)\n","        # print(f\"Output of conv & max pool 2: {x_.shape}\")\n","\n","        x_ = self.conv_3(x_)\n","        x_ = self.max_pool_3(x_)\n","        # print(f\"Output of conv & max pool 3: {x_.shape}\")\n","\n","        x_ = self.conv_4(x_)\n","        x_ = self.max_pool_4(x_)\n","        # print(f\"Output of conv & max pool 4: {x_.shape}\")\n","\n","        x_ = self.flatten(x_)\n","        # print(f\"Output of flatten: {x_.shape}\")\n","\n","        x_ = self.projection(x_)\n","        # print(f\"Output of projection: {x_.shape}\")\n","\n","        output = []\n","        for i, layer in enumerate(self.linear_layers):\n","            # print(f\"Getting slice: {2**(3+i)} - {x_[:, 0:2**(3+i)].shape}\")\n","            x__ = layer(x_[:, 0:2**(3+i)])\n","            # print(f\"Subset output: {x__.shape}\")\n","            output.append(x__)\n","\n","        output = torch.stack(output, dim=1)\n","        # print(f\"Final output: {output.shape}\")\n","        return output\n","\n","def create_model(in_dimensions: int, dims: int, num_labels: int):\n","    \"\"\"\n","    Create model\n","    \"\"\"\n","\n","    model = AnimalClassifierMatryoshka(in_dimensions, dims, num_labels)\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["## Train Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:24:08.441112Z","iopub.status.busy":"2024-02-26T23:24:08.440462Z","iopub.status.idle":"2024-02-26T23:24:08.472217Z","shell.execute_reply":"2024-02-26T23:24:08.471086Z","shell.execute_reply.started":"2024-02-26T23:24:08.441075Z"},"trusted":true},"outputs":[],"source":["def compute_loss(preds: torch.Tensor, labels: torch.Tensor,\n","                 weights: torch.Tensor):\n","    \"\"\"\n","    Compute loss\n","    \"\"\"\n","\n","    # Sum over each subset & average over each batch\n","    loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n","    \n","    # Cross entropy loss require (batch_size, num_classes, ...)\n","    preds_ = torch.transpose(preds, 1, 2)\n","    # print(f\"Transposed predictions: {pred_.shape}\")\n","\n","    output = loss_fn(preds_, labels)\n","    if weights is not None:\n","        output = weights*output\n","    # print(f\"Output: {output.shape}\")\n","\n","    loss = output.sum(dim=1).mean()\n","    return loss\n","\n","@torch.no_grad()\n","def eval_loop(epoch: int, model, dataloader,\n","              num_subsets: int, wandb_run, accelerator: Accelerator):\n","    \"\"\"\n","    Evaluation loop\n","    \"\"\"\n","\n","    tensor_to_pil = transforms.ToPILImage()\n","    columns = [f'pred_{i}' for i in range(num_subsets)]\n","\n","    dataframe = []\n","    original_images = []\n","    images = []\n","    gt = []\n","    \n","    avg_loss = 0\n","    for i, batch in enumerate(dataloader):\n","\n","        logits = model(batch['image'])\n","        preds = torch.argmax(logits, dim=-1)\n","\n","        labels = torch.stack(\n","            [batch['label'] for _ in range(logits.shape[1])])\n","        labels = torch.transpose(labels, 1, 0)\n","\n","        loss = compute_loss(logits, labels, None)            \n","        avg_loss += loss.item()\n","\n","        # acc = (preds == labels).double()\n","        # wandb_run.log({'accuracy': acc.mean()}, commit=False)\n","        # wandb_run.log({'val-loss': loss.item()}, commit=False)\n","        \n","        _images = []\n","        _original_images = []\n","        for j in range(batch['image'].shape[0]):\n","            _images.append(tensor_to_pil(batch['image'][j,:]))\n","            _original_images.append(tensor_to_pil(batch['original-image'][j,:]))\n","\n","        images += _images\n","        original_images += _original_images\n","\n","        dataframe += preds.tolist()\n","        gt += batch['label'].tolist()\n","\n","    dataframe = pd.DataFrame(dataframe,\n","                             columns=columns)\n","    dataframe['epoch'] = epoch\n","    dataframe['image'] = \\\n","        [wandb.Image(image) for image in images]\n","    dataframe['original_images'] = \\\n","        [wandb.Image(image) for image in original_images]\n","    dataframe['gt'] = gt\n","\n","    # Get average accuracy and loss\n","    for i in range(num_subsets):\n","        acc = (dataframe['gt'] == dataframe[f'pred_{i}']).mean()\n","        accelerator.print(\n","            f\"Val accuracy for subset {i+1}: {acc}\")\n","        wandb_run.log({f'acc-subset-{i+1}': acc}, commit=False)\n","\n","    avg_loss = avg_loss/len(dataloader)\n","    accelerator.print(f\"Val loss: {avg_loss}\")\n","\n","    table = wandb.Table(data=dataframe)\n","    wandb_run.log({'val-loss': avg_loss}, commit=False)\n","    wandb_run.log({'eval-table': table})\n","\n","def training_loop(config: Namespace):\n","    \"\"\"\n","    Training loop\n","    \"\"\"\n","\n","    wandb_run = wandb.init(project='Animal-Classifier', entity=None,\n","                           job_type='training',\n","                           name=config.run_name,\n","                           config=config)\n","\n","    set_seed(config.seed)\n","\n","    grad_accumulation_plugin = GradientAccumulationPlugin(\n","        num_steps=config.grad_accumulation_steps,\n","        adjust_scheduler=True,\n","        sync_with_dataloader=True)\n","\n","    accelerator = Accelerator(\n","        mixed_precision=config.mixed_precision,\n","        gradient_accumulation_plugin=grad_accumulation_plugin,\n","        cpu=(config.device == 'cpu'))\n","\n","    train_dataloader, val_dataloader = prepare_dataloader(config)    \n","    model = create_model(3, config.hidden_dims, 2)\n","    num_subsets = len(model.linear_layers)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n","\n","    # scheduler = CosineAnnealingLR(\n","    #     optimizer,\n","    #     T_max=config.num_train_epochs)\n","    scheduler = ExponentialLR(\n","        optimizer,\n","        config.lr_exp_schedule_gamma)\n","    # scheduler = CosineAnnealingWarmRestarts(\n","    #     optimizer,\n","    #     T_0=config.lr_warmup_steps)\n","    #     # last_epoch=config.num_train_epochs*len(train_dataloader))\n","\n","    model, optimizer, train_dataloader, val_dataloader, scheduler = accelerator.prepare(\n","        model, optimizer, train_dataloader, val_dataloader, scheduler)\n","\n","    num_steps = 0\n","    for epoch in range(config.num_train_epochs):\n","        model.train()\n","\n","        accelerator.print(f\"Epoch {epoch}\")\n","\n","        epoch_loss = 0\n","        num_iters = 0\n","\n","        for _, batch in enumerate(train_dataloader):\n","            with accelerator.accumulate(model):\n","\n","                optimizer.zero_grad()\n","                logits = model(batch['image'])\n","                labels = torch.stack(\n","                    [batch['label'] for _ in range(logits.shape[1])])\n","                labels = torch.transpose(labels, 1, 0)\n","\n","                loss = compute_loss(logits, labels, None)            \n","                accelerator.backward(loss)\n","                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n","\n","                epoch_loss += loss.item()\n","\n","                wandb_run.log({'loss': loss.item()}, commit=False, step=num_steps)\n","                wandb_run.log({'lr': scheduler.get_lr()[0]}, commit=False, step=num_steps)\n","\n","                num_steps += 1\n","                num_iters += 1\n","\n","                # Update the model parameters with the optimizer\n","                optimizer.step()\n","        scheduler.step()\n","\n","        # Validate model\n","        accelerator.print(\"Evaluating model\")\n","        eval_loop(epoch, model, val_dataloader, num_subsets, wandb_run, accelerator)\n","\n","        wandb_run.log({'epoch-loss': epoch_loss/num_iters})\n","\n","    if config.save_model:\n","        # Save model to W&Bs\n","        model_art = wandb.Artifact(config.model_name, type='model')\n","        torch.save(model.state_dict(), 'model.pt')\n","\n","        model_art.add_file('model.pt')\n","        wandb_run.log_artifact(model_art)\n","    wandb_run.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:24:08.473761Z","iopub.status.busy":"2024-02-26T23:24:08.473440Z","iopub.status.idle":"2024-02-26T23:24:08.487210Z","shell.execute_reply":"2024-02-26T23:24:08.486306Z","shell.execute_reply.started":"2024-02-26T23:24:08.473728Z"},"trusted":true},"outputs":[],"source":["# # For debugging\n","# ACCELERATOR = Accelerator(\n","#     mixed_precision=CONFIG.mixed_precision,\n","#     cpu=(CONFIG.device == 'cpu'))\n","\n","# MODEL = create_model(3, CONFIG.hidden_dims, 2)\n","# train_dataloader, val_dataloader = prepare_dataloader(CONFIG)\n","# eval_loop(0, MODEL, val_dataloader, len(MODEL.linear_layers), None, ACCELERATOR)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:24:08.488800Z","iopub.status.busy":"2024-02-26T23:24:08.488403Z","iopub.status.idle":"2024-02-27T03:41:31.496747Z","shell.execute_reply":"2024-02-27T03:41:31.495880Z","shell.execute_reply.started":"2024-02-26T23:24:08.488764Z"},"trusted":true},"outputs":[],"source":["from accelerate import notebook_launcher\n","\n","notebook_launcher(training_loop, (CONFIG, ), num_processes=1)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
